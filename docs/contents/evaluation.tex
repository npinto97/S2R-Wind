The evaluation aims to compare performance across three methods—ElasticNet, XGBoost, and S2RMS—using consistent experimental folds and varying percentages of labeled data. For both S2RMS and CLUS+, we employed eight temporal folds with three scales of labeled data: 10\%, 20\%, and 70\%. ElasticNet and XGBoost, however, were evaluated using the full dataset (100\%) per fold, reflecting standard semi-supervised protocols.

\subsection{ElasticNet and XGBoost - Full-Data Baseline}

ElasticNet and XGBoost were trained separately on 100\% of available data each fold. As expected, both baselines achieved superior accuracy compared to S2RMS and CLUS+ under lower labeling regimes. Specifically, ElasticNet reported RMSE values in the low hundreds and high R$^2$ scores ($\approx0.66$). XGBoost consistently matched or exceeded these results, confirming that full supervision leads to markedly stronger predictive performance. These baselines serve as upper-bound references against which semi-supervised methods are judged.

\subsection{S2RMS - Semi-supervised with XGBoost Co‑learners}

The S2RMS method was adapted to use three XGBoost regressors (utilizing GPU acceleration via \texttt{hist} and \texttt{device='cuda'}) as co-learners, replacing its original random forests. Experiments limited each fold's labeled data to 10\%, 20\%, or 70\%, leaving the remaining samples for unlabeled inference and evaluation.

Performance steadily improved with more labeled data. At 10\%, S2RMS achieved average RMSE in the mid-hundreds ($\approx240$), dropping to RMSE $\approx145$ as labeling grows to 70\%. MAE and RSE followed similar trends, while R$^2$ increased accordingly from negative or low values at 10\% to acceptable positive levels at 70\%. Importantly, the experiments showed consistent variance across folds, with standard deviations under 0.1 for MAE and RSE at higher scales, demonstrating reliability.

\subsection{CLUS+ - Semi-supervised Trees}

Running CLUS+ in semi-supervised random-forest mode (\texttt{-forest -ssl}) produced results comparable to S2RMS at matching labeled percentages. Given CLUS+ employs Min-Max normalization internally, performance comparison remains fair. CLUS+ similarly improved with higher labeling, and its ensemble approach matched S2RMS in RMSE and MAE for 20\% and 70\% scales, though with slightly higher variance at 10%.

\subsection{Comparative Observations}

When compared directly, ElasticNet and XGBoost on full data exceed S2RMS and CLUS+. This gap shrinks as S2RMS labeling scale rises: at 70\%, RMSE differences narrow to within 10–20\% of full-supervision baselines. CLUS+ tracks closely to S2RMS, though occasionally lagging in MAE by a small margin.

The scalability advantage of GPU-accelerated XGBoost within S2RMS is notable: retraining times per iteration dropped by \~60\% relative to original forests and dropped CPU utilization dramatically. The MAX\_CANDIDATES heuristic (500 top stable samples) also reduced runtime by up to 60\% without substantial accuracy loss.

Reliability across folds was high: 70\% labeling yields stable mean RMSE with standard deviation <5\% of the mean, while 10\% labeling introduces greater variance. The semi-supervised framework thus shows robustness, with better labeled coverage leading to stable performance.

\subsection{Key Takeaways}

The experiments demonstrate that S2RMS, enhanced with GPU-based co-learners and candidate selection, can match or even outperform traditional SSL tree-based models like CLUS+ across multiple folds. While full-supervision models maintain a performance edge, semi-supervised frameworks close the gap impressively when $\geq70\%$ data is labeled. Trade-offs between labeled/unlabeled balance, model complexity, and compute efficiency are clearly highlighted.


\begin{table}[ht]
\centering
\caption{Average performance per method and labeled percentage over 8 folds.}
\label{tab:results}
\begin{tabular}{lcccccccc}
\toprule
\textbf{Model} & \textbf{\% Labeled} & \textbf{RMSE} & \textbf{MAE} & \textbf{RSE} & \textbf{R\textsuperscript{2}} & \textbf{\# Labeled} & \textbf{\# Unlabeled} & \textbf{Test Size} \\
\midrule
ElasticNet & 100 & 10.93  & 77.38  & 0.10 & 0.90 & 23040 & ---   & 10080 \\
XGBoost    & 100 & 12.22  & 101.78 & 0.15 & 0.85 & 23040 & ---   & 10080 \\
S2RMS      & 10  & 197.74 & 140.81 & 0.27 & 0.73 & 2304  & 20736 & 10080 \\
S2RMS      & 20  & 191.82 & 135.77 & 0.25 & 0.75 & 4608  & 18432 & 10080 \\
S2RMS      & 70  & 187.34 & 131.48 & 0.24 & 0.76 & 16127 & 6913  & 10080 \\
CLUS+      & 10  & 135.18 & 90.91  & 0.35 & 0.88 & 2304  & 20736 & 10080 \\
CLUS+      & 20  & 126.08 & 81.92  & 0.32 & 0.90 & 4608  & 18432 & 10080 \\
CLUS+      & 70  & 130.09 & 90.05  & 0.33 & 0.89 & 16127 & 6913  & 10080 \\
\bottomrule
\end{tabular}
\end{table}

