The raw SCADA dataset underwent a structured transformation pipeline to prepare it for both supervised and semi-supervised regression tasks. This process involved time-based feature engineering, instance windowing, normalization, and formatting into formats compatible with distinct modeling environments, such as Python-based libraries (e.g., scikit-learn, XGBoost) and the Java-based CLUS+ framework.

A critical step in preprocessing was the construction of lagged features: for each instance, the 12 preceding values of \texttt{patv\_target\_1} were appended as predictors, yielding a temporally contextualized representation of turbine output. The prediction target was the next time step (i.e., 10 minutes ahead), preserving the causal structure essential for real-time forecasting applications. These feature transformations were computed independently per turbine to maintain intra-series temporal coherence.

From the processed data, two parallel sets of folds were derived. The first set, comprising scaled folds, was used for all Python-based methodsâ€”ElasticNet, XGBoost, and S2RMS. Here, both input features and targets were normalized using a \texttt{StandardScaler}, fitted on the training portion of each fold. These scalers were stored and reused consistently for testing and for inverse transforming predictions during evaluation. The second set, based on unscaled data, was generated specifically for CLUS+, which performs its own internal normalization during model induction. This separation ensures consistency with the modeling assumptions of each framework.

Each fold consists of a 15-day training window and a subsequent 7-day test window. Within each training fold, three supervised signal levels were simulated by marking 10%, 20%, or 70% of the training data as labeled. For semi-supervised configurations, the remaining samples were considered unlabeled. In the CLUS+ folds, unlabeled examples were explicitly encoded by replacing their target values with \texttt{?}, allowing CLUS+ to interpret them as partially observed during the construction of Predictive Clustering Trees (PCTs). For S2RMS, labeled and unlabeled examples were kept distinct and processed through a dedicated pseudo-labeling pipeline during training.

All folds were exported in both CSV and ARFF formats. CSV was used exclusively with ElasticNet and XGBoost, while ARFF was required for both CLUS+ and S2RMS. The dual-format export and fold parallelism ensured that all models were trained and evaluated on an identical temporal and structural basis, thus enabling fair comparative analysis.
