The dataset used in this study is derived from the \textit{Baidu KDD Cup 2022} wind power forecasting competition. It comprises SCADA sensor readings collected from multiple wind turbines at 10-minute intervals over 245 consecutive days. Each turbine is uniquely identified and provides features such as wind speed, wind direction, ambient temperature, rotor speed, and power output (PATV), in addition to timestamp and geolocation attributes.

To reduce computational and memory overhead, we restrict our analysis to 10 turbines. For temporal generalization, we generate 8 folds, each consisting of a 15-day training period and a 7-day test period. These folds are constructed using a sliding window approach to simulate realistic forecasting on unseen future data.

Each learning instance includes a fixed-length window of historical values (\texttt{n\_hist\_steps = 12}) and is used to predict the power output 10 minutes ahead (\texttt{n\_targets = 1}). Additional temporal context features such as hour of day and day of the week are derived from the timestamps. For each fold, training and test sets are generated independently, and instances with missing values are discarded to ensure model compatibility.

For Python-based experiments (S2RMS, ElasticNet, XGBoost), features are standardized using \texttt{StandardScaler}, and the target variable is normalized independently using the same method. Only a fraction of the training set is labeled (10\%, 20\%, or 70\%) with the remaining instances used as unlabeled data. These are passed to semi-supervised models with masked targets or omitted labels, depending on the method.

For experiments with CLUS+, we use the same folds, but without applying external normalization. CLUS+ handles normalization internally via its built-in \texttt{MinMaxNormalization} method, which scales each feature to the $ [0,1] $ range by default. This ensures consistency across CLUS+ runs, although it differs slightly from the \texttt{StandardScaler} applied in the Python pipelines. Training data for CLUS+ is provided in ARFF format, with labeled and unlabeled examples distinguished by missing target values (\texttt{?}), in line with CLUS+'s semi-supervised configuration.